{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fykhFKa3Ulnh"
      },
      "source": [
        "# ECE 9309/9039 Assignment # 2 Winter 2023\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "370AKC5oUxsB"
      },
      "source": [
        "This notebook contains the questions for Assignment 2. Please note, a random seed has been set to ensure the reproducibility of the results -- *DO NOT* change this random seed. **If you call additional functions that are based on random number generators, you will need to define their seed to 42 as well**. Make sure to complete this assignment individually and appropriately reference all external code and documentation used. ***In order for your submission to be valid, you must adhere to the function definitions which have been made (failure to do so will result in a grade of 0). You must upload this completed Jupyter Notebook file as your submission (other file types are not permitted and will result in a grade of 0).*** You are responsible for selecting and importing additional packages. Your functions ***must*** be self-contained and should not reference or change global variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "45VAP00lUkc5"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "np.random.seed(42)\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.ensemble import RandomForestClassifier,BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import classification_report, accuracy_score, f1_score, recall_score, precision_score, confusion_matrix\n",
        "import time\n",
        "\n",
        "#add other imports here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3YmD_jy2VkV5"
      },
      "source": [
        "## Dataset Description\n",
        "\n",
        "Attached with the assignment instructions, you will find the datasets.zip file. After unzipping the file, you will find two .csv files, where each file represents real-world measurement data of a heat experiment inside a steel furnace. \"merged_exp_normal.csv\" has all the normal experimental samples, while \"merged_exp_anomalous.csv\" has all the abnormal experimental samples.\n",
        "\n",
        "In the datasets, the features are the vibration measurements in columns A, B, ... , H, which correspond to (X1, X2, ... , X8) measurement signals. Each feature represents a vibration signal inside the furnace at several frequency bands. Each example is a measurement recorded at a time instance (Timestamp), which are considered time-series data measurements."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UD1u-9MwVY5k"
      },
      "source": [
        "## Question 1 - Data Preparations [10 Marks]\n",
        "\n",
        "A) Read \"merged_exp_normal.csv\" as a pandas dataframe \"normalMergedData\", and print out the shape of the merged normal dataset.\n",
        "\n",
        "B) Read \"merged_exp_anomalous.csv\" as a pandas dataframe \"anomalousMergedData\", and print out the shape of the merged anomalous dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "VYa70oGzXmbl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1373ec9c-a1ad-49a8-9198-06e1b31e3027"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(76593, 10)\n"
          ]
        }
      ],
      "source": [
        "### Q1A)\n",
        "#Read and print out the shape of the dataset\n",
        "normalMergedData = pd.read_csv('merged_exp_normal.csv')\n",
        "print(normalMergedData.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "T2EbZ-FwYIGl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "11e55c9c-7f81-41ab-bb57-288230887a82"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2599, 10)\n"
          ]
        }
      ],
      "source": [
        "### Q1B)\n",
        "#Read and print out the shape of the dataset\n",
        "anomalousMergedData = pd.read_csv('merged_exp_anomalous.csv')\n",
        "print(anomalousMergedData.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A2LBEIAJYQV1"
      },
      "source": [
        "## Question 2 - Supervised Learning Algorithms [50 Marks]\n",
        "\n",
        "A) Complete the myTrainTestSplit function, which takes as input **two dataframes** consisting of the normal and anomalous datasets and **returns 4 variables, Xtrain, Xtest, ytrain, and ytest**. Reserve 30 percent of your data for testing. Please consider ensuring the **same distribution** of anomalous data is present in both training and test sets when splitting the anomalous datasets. \n",
        "\n",
        "*Note: you can use `concat` from pandas library to concatenade trainig sets from two dataframes, and test sets from two dataframes.*\n",
        "\n",
        "\n",
        "B) Apply a Decision Tree model for classifying the events as normal or anomalous. Fill in the myDecisionTree function, which accepts as input the training set and returns a fully trained model. \n",
        "\n",
        "C) Apply a Bagging model that consists of 10 base decision trees for classifying the events as normal or anomalous. Fill in the myBagging function, which accepts as input the training set and returns a fully trained model. \n",
        "\n",
        "D) Apply a Random Forest model that consists of 10 base decision trees for classifying the events as normal or anomalous. Fill in the myRandomForest function, which accepts as input the training set and returns a fully trained model. \n",
        "\n",
        "E) Based on the trained Random Forest model from 2D, use a horizontal bar plot to plot the feature importance scores of all features (Timestamp, X1-X8).\n",
        "\n",
        "F) Fill in the myEvaluateSupervisedModelPerformance function, which takes as input the training and test sets. Please note, you will need to call your functions from 2B, 2C, and 2D within this function to train your models. \n",
        "- Calculate the training time for comparing models. \n",
        "- Evaluate the performance of your models using 3 metrics of `Recall`, `Preceision` and `f1 score`. Return three numpy arrays consisting of the three metrics calculated for each model on the test set and print them out. \n",
        "- Also, print out the confusion matrix of your three models on the test set. \n",
        "\n",
        "G) Written answer - Use the markdown cell to answer the following:\n",
        "- Justify the choice of three metrics for the evaluation of the model's performance.\n",
        "- Compare the performance and training time of the Decision Tree and Bagging models and discuss the reasons for the performance difference.\n",
        "- Compare the performance and training time of the Bagging and Random Forest models and discuss the reasons for the performance difference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "3cdpWFuVZPFl"
      },
      "outputs": [],
      "source": [
        "### Q2A)\n",
        "\n",
        "def myTrainTestSplit(normalData, anomalousData):\n",
        "  #write function here\n",
        "  test_size=0.3;\n",
        "  random_state=42;\n",
        "  X_train_norm, X_test_norm, y_train_norm, y_test_norm = train_test_split(normalData.drop('Anomaly_Tag', axis=1), normalData['Anomaly_Tag'], test_size=test_size, random_state=random_state)\n",
        "  X_train_anom, X_test_anom, y_train_anom, y_test_anom = train_test_split(anomalousData.drop('Anomaly_Tag', axis=1), anomalousData['Anomaly_Tag'], test_size=test_size, random_state=random_state)\n",
        "\n",
        "  # Combine the normal and anomalous training and testing sets\n",
        "  Xtrain = pd.concat([X_train_norm, X_train_anom], axis=0)\n",
        "  Xtest = pd.concat([X_test_norm, X_test_anom], axis=0)\n",
        "  ytrain = pd.concat([y_train_norm, y_train_anom], axis=0)\n",
        "  ytest = pd.concat([y_test_norm, y_test_anom], axis=0)\n",
        "    \n",
        "  return Xtrain, Xtest, ytrain, ytest\n",
        "\n",
        "#Call the function here\n",
        "Xtrain, Xtest, ytrain, ytest= myTrainTestSplit(normalMergedData, anomalousMergedData);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "Rdw3xDzGZdAb"
      },
      "outputs": [],
      "source": [
        "### Q2B)\n",
        "\n",
        "def myDecisionTree(Xtrain, ytrain):\n",
        "  #write function here\n",
        "  myDecisionTree = DecisionTreeClassifier(random_state=42)\n",
        "  myDecisionTree.fit(Xtrain, ytrain)\n",
        "    \n",
        "  return myDecisionTree"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "7nv9wTMAaKzq"
      },
      "outputs": [],
      "source": [
        "### Q2C)\n",
        "\n",
        "def myBagging(Xtrain, ytrain):\n",
        "  #write function here\n",
        "  myBagging = BaggingClassifier(estimator=DecisionTreeClassifier(random_state=42), n_estimators=10, random_state=42)\n",
        "  myBagging.fit(Xtrain, ytrain)\n",
        "  return myBagging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "IFMi4Lxna1i0"
      },
      "outputs": [],
      "source": [
        "### Q2D)\n",
        "\n",
        "def myRandomForest(Xtrain, ytrain):\n",
        "  #write function here\n",
        "  myRandomForest = RandomForestClassifier(n_estimators=10, random_state=42)\n",
        "  myRandomForest.fit(Xtrain, ytrain)\n",
        "  return myRandomForest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "5WzBCDpha1i0",
        "outputId": "b10461f1-613a-4c76-b6c8-c275b668f283"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "### Q2E)\n",
        "import matplotlib.pyplot as plt\n",
        "def myRandomForestFeatureImportance(myRandomForestModel):\n",
        "  importances = myRandomForestModel.feature_importances_  \n",
        "  feature_names = list(Xtrain.columns)\n",
        "  # Create a horizontal bar plot\n",
        "  fig, ax = plt.subplots()\n",
        "  ax.barh(feature_names, importances)\n",
        "  ax.invert_yaxis()\n",
        "  ax.set_title('Feature importance scores of all features')\n",
        "  ax.set_xlabel('Feature Importance')\n",
        "  ax.set_ylabel('Feature Names')\n",
        "  plt.show()\n",
        "    \n",
        "#Call the function here\n",
        "myRandomForestFeatureImportance(myRandomForest(Xtrain, ytrain))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "ldPs5xBRa7Q5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66716334-9249-44a8-e4ee-f53a4dbbd400"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "myDecisionTree_training_time:  0.4558906555175781\n",
            "myBagging_training_time:  0.4558906555175781\n",
            "myRandomForest_training_time:  0.4558906555175781 \n",
            "\n",
            "DT_Results:  [0.9141025641025641, 0.8991172761664565, 0.9065479974570884]\n",
            "Bagging_Results:  [0.9217948717948717, 0.9809004092769441, 0.9504296100462657]\n",
            "RF_Results:  [0.882051282051282, 0.9927849927849928, 0.9341479972844535] \n",
            "\n",
            "confusion Matrix of  DecisionTreeClassifier(random_state=42) \n",
            " [[22898    80]\n",
            " [   67   713]] \n",
            "\n",
            "confusion Matrix of  BaggingClassifier(estimator=DecisionTreeClassifier(random_state=42),\n",
            "                  random_state=42) \n",
            " [[22964    14]\n",
            " [   61   719]] \n",
            "\n",
            "confusion Matrix of  RandomForestClassifier(n_estimators=10, random_state=42) \n",
            " [[22973     5]\n",
            " [   92   688]] \n",
            "\n"
          ]
        }
      ],
      "source": [
        "### Q2F)\n",
        "import time\n",
        "def myEvaluateSupervisedModelPerformance(Xtrain, Xtest, ytrain, ytest):\n",
        "  #write function here\n",
        "  #Calculate the training time for comparing models.\n",
        "  start_time = time.time();\n",
        "  myDecisionTree(Xtrain, ytrain);\n",
        "  end_time = time.time();\n",
        "  myDecisionTree_training_time = end_time - start_time;\n",
        "  print(\"myDecisionTree_training_time: \",  myDecisionTree_training_time)\n",
        "  \n",
        "  start_time = time.time();\n",
        "  myBagging(Xtrain, ytrain);\n",
        "  end_time = time.time();\n",
        "  myBagging_training_time = end_time - start_time;\n",
        "  print(\"myBagging_training_time: \",  myDecisionTree_training_time)\n",
        "\n",
        "  start_time = time.time();\n",
        "  myRandomForest (Xtrain, ytrain);\n",
        "  end_time = time.time();\n",
        "  myRandomForest_training_time = end_time - start_time;\n",
        "  print(\"myRandomForest_training_time: \",  myDecisionTree_training_time, \"\\n\")\n",
        " \n",
        "  #Evaluate the performance of  models using 3 metrics of Recall, Preceision and f1 score.\n",
        "  \n",
        "  ypred = myDecisionTree(Xtrain,ytrain).predict(Xtest)\n",
        "  DT_recall = recall_score(ytest, ypred)\n",
        "  DT_precision = precision_score(ytest, ypred)\n",
        "  DT_f1 = f1_score(ytest, ypred)\n",
        "  DT_Results=[DT_recall, DT_precision, DT_f1 ]\n",
        "\n",
        "  ypred = myBagging(Xtrain,ytrain).predict(Xtest)\n",
        "  Bagging_recall = recall_score(ytest, ypred)\n",
        "  Bagging_precision = precision_score(ytest, ypred)\n",
        "  Bagging_f1 = f1_score(ytest, ypred)\n",
        "  Bagging_Results=[Bagging_recall, Bagging_precision, Bagging_f1 ]\n",
        "\n",
        "  ypred = myRandomForest(Xtrain,ytrain).predict(Xtest)\n",
        "  RF_recall = recall_score(ytest, ypred)\n",
        "  RF_precision = precision_score(ytest, ypred)\n",
        "  RF_f1 = f1_score(ytest, ypred)\n",
        "  RF_Results=[RF_recall, RF_precision, RF_f1 ]\n",
        "\n",
        "  return DT_Results, Bagging_Results, RF_Results\n",
        "\n",
        "#Call the function here\n",
        "DT_Results, Bagging_Results, RF_Results=myEvaluateSupervisedModelPerformance(Xtrain, Xtest, ytrain, ytest)\n",
        "print(\"DT_Results: \", DT_Results)\n",
        "print(\"Bagging_Results: \", Bagging_Results)\n",
        "print(\"RF_Results: \", RF_Results, \"\\n\")\n",
        "\n",
        "#print out the confusion matrix of the three models on the test set.\n",
        "def printConfusionMatrix(model):\n",
        "  ypred=model.predict(Xtest)\n",
        "  cm= confusion_matrix(ytest,ypred)\n",
        "  print(\"confusion Matrix of \",model,\"\\n\" ,cm, \"\\n\")\n",
        "\n",
        "printConfusionMatrix(myDecisionTree(Xtrain,ytrain))\n",
        "printConfusionMatrix(myBagging(Xtrain,ytrain))\n",
        "printConfusionMatrix(myRandomForest(Xtrain,ytrain))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ttLtyLi1b0IE"
      },
      "source": [
        "#### Q2G)\n",
        "\n",
        "## Choice of three metrics for the evaluation of the model's performance.\n",
        "Recall: Recall measures the proportion of actual positive cases that were correctly identified by the model.\n",
        "\n",
        "Precision: Precision measures the proportion of predicted positive cases that were actually positive. \n",
        "\n",
        "F1 score: The F1 score is the harmonic mean of precision and recall.\n",
        "\n",
        "The choice of recall, precision, and F1 score as metrics for evaluating the model's performance is appropriate because they provide a well-rounded view of the model's performance\n",
        "\n",
        "\n",
        "##Comparison of  the performance and training time of the Decision Tree and Bagging models \n",
        "\n",
        "\n",
        "\n",
        "*   The training time is same for both the models \n",
        "\n",
        "\n",
        "*  Performance comparison:\n",
        "\n",
        "*Decision tree model:*\n",
        "\n",
        "Recall = 0.9141025641025641\n",
        "Precision = 0.8991172761664565\n",
        "F1 score = 0.9065479974570884\n",
        "\n",
        "*Bagging model:*\n",
        "\n",
        "Recall = 0.9217948717948717\n",
        "Precision = 0.9809004092769441\n",
        "F1 score = 0.9504296100462657\n",
        "\n",
        "we can see that the bagging model outperformed the decision tree model in terms of precision, recall, and F1 score. This indicates that it is a better model for this problem.\n",
        "\n",
        "The reason for the performance difference could be that Decision trees have a tendency to overfit the training data, which can result in poor performance on the test data. Bagging, on the other hand, helps to reduce overfitting by training multiple trees on different subsets of the training data and then aggregating the results.\n",
        "\n",
        "##Compare the performance and training time of the Bagging and Random Forest models\n",
        "\n",
        "\n",
        "*   The training time is same for both the models \n",
        "\n",
        "*   Performance comparison:\n",
        "\n",
        "Bagging model: \n",
        "\n",
        "Recall = 0.9217948717948717\n",
        "Precision = 0.9809004092769441\n",
        "F1 score = 0.9504296100462657\n",
        "\n",
        "Random forest model:\n",
        "\n",
        "Recall = 0.882051282051282\n",
        "Precision = 0.9927849927849928\n",
        "F1 score = 0.9341479972844535\n",
        "\n",
        "Based on the results reported, the bagging model performs better than the random forest model in terms of overall F1 score, which is often a good metric to use for imbalanced datasets. However, the random forest model has a higher precision, which could be important if avoiding false positives is a primary concern.\n",
        "\n",
        "The reason for performance difference could be that Random forests use multiple decision trees with additional randomness to further reduce overfitting. This increases complexity  and can sometimes result in better precision but could also lead to overfitting and reduced performance.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "buQSFi6rb7yE"
      },
      "source": [
        "## Question 3 - Unsupervised Learning Algorithms [40 Marks]\n",
        "\n",
        "A) Apply a KMeans model to decouple the anomalous data from the normal data. Fill in the myKMeans function which accepts as input the training set and returns a fully trained clustering model.\n",
        "\n",
        "B) Build the pipeline that uses a Principal Component Analysis (PCA) model to extract 2 principal components of the training set and create a a KMeans model (same as the model in 3A). Fill in the myPCAKMeans function, which accepts as input the training set and returns a fully trained model. \n",
        "\n",
        "C) Build the pipeline that uses a Principal Component Analysis (PCA) model to extract 2 principal components of the training set and create a a Random Forest model that consists of 50 base decision trees (same as the model in 2D).  Fill in the myPCARF function, which accepts as input the training set and returns a fully trained model. \n",
        "\n",
        "D) Fill in the myEvaluateUnsupervisedModelPerformance function, which takes as input the training and test sets. Please note, you will need to call your functions from 3A, 3B, and 3C within this function to train your models. \n",
        "- Calculate the training time for comparing models. \n",
        "- Evaluate the performance of your models on the test set using the 3 same metrics from 2F. Return three numpy arrays consisting of the three metrics calculated for each model and print them out. \n",
        "- Also, print out the confusion matrix of your three models on the test set. \n",
        "\n",
        "*Note: This function can be the same as the function from 2F, but remember to change all the model names.*\n",
        "\n",
        "E) Written answer - Use the markdown cell to answer the following:\n",
        "- Compare the performance and training time of KMeans model and the Random Forest model from 2D and discuss the reasons for the performance difference.\n",
        "- Compare the performance and training time of PCA + Random Forest model from 3C and the Random Forest model from 2D, and discuss the reasons for the performance difference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "DePEtIW3dVHl"
      },
      "outputs": [],
      "source": [
        "### Q3A) \n",
        "def myKMeans(Xtrain, ytrain):\n",
        "  myKMeans = KMeans(n_clusters=2)\n",
        "  myKMeans.fit(Xtrain)\n",
        "  return myKMeans"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "qbKrpYO8a1i1"
      },
      "outputs": [],
      "source": [
        "### Q3B) \n",
        "def myPCAKMeans(Xtrain, ytrain):\n",
        "    myPCAKMeansModel = Pipeline([\n",
        "        (\"pca\", PCA(n_components=2)),\n",
        "        (\"kmeans\", KMeans(n_clusters=2, random_state=42))])\n",
        "    myPCAKMeansModel.fit(Xtrain)\n",
        "    return myPCAKMeansModel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "c7KcEibrdVeG"
      },
      "outputs": [],
      "source": [
        "### Q3C)\n",
        "def myPCARF(Xtrain, ytrain):\n",
        "    #write function here \n",
        "    myPCARF = Pipeline([\n",
        "        ('pca', PCA(n_components=2)),\n",
        "        ('rf', RandomForestClassifier(n_estimators=50))])\n",
        "    myPCARF.fit(Xtrain,ytrain )\n",
        "    return myPCARF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "asKST7s6a1i1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f16a2811-36ca-420b-da95-e3630fed1e4d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KMeans  training time:  2.2316997051239014 \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PCAKMeans  training time:  1.2275090217590332 \n",
            "\n",
            "PCARF  training time:  2.909113883972168 \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KMeans_Results:  [0.15, 0.012508017960230917, 0.023090586145648313]\n",
            "PCAKMeans_Results:  [0.85, 0.04607046070460705, 0.08740359897172237]\n",
            "PCARF_Results:  [0.4371794871794872, 0.6595744680851063, 0.525828835774865] \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "confusion Matrix of  KMeans(n_clusters=2) \n",
            " [[13775  9203]\n",
            " [  668   112]] \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "confusion Matrix of  Pipeline(steps=[('pca', PCA(n_components=2)),\n",
            "                ('kmeans', KMeans(n_clusters=2, random_state=42))]) \n",
            " [[ 9250 13728]\n",
            " [  117   663]] \n",
            "\n",
            "confusion Matrix of  Pipeline(steps=[('pca', PCA(n_components=2)),\n",
            "                ('rf', RandomForestClassifier(n_estimators=50))]) \n",
            " [[22800   178]\n",
            " [  438   342]] \n",
            "\n"
          ]
        }
      ],
      "source": [
        "### Q3D)\n",
        "\n",
        "# function to calculate the training time for each model\n",
        "def calculateTrainingTime(modelName):\n",
        "  start_time = time.time();\n",
        "\n",
        "  if modelName== \"KMeans\":\n",
        "    myKMeans(Xtrain,ytrain)\n",
        "  elif modelName== \"PCAKMeans\":\n",
        "    myPCAKMeans(Xtrain,ytrain)\n",
        "  elif modelName==\"PCARF\":\n",
        "    myPCARF(Xtrain,ytrain)\n",
        "\n",
        "  end_time = time.time();\n",
        "  training_time = end_time - start_time;\n",
        "  print(modelName,\" training time: \",  training_time, \"\\n\")\n",
        "\n",
        "def myEvaluateUnsupervisedModelPerformance(Xtrain, Xtest, ytrain, ytest):\n",
        "  #write function here\n",
        "  #Calculate the training time for comparing models.\n",
        "  calculateTrainingTime(\"KMeans\")\n",
        "  calculateTrainingTime(\"PCAKMeans\")\n",
        "  calculateTrainingTime(\"PCARF\")\n",
        "\n",
        "  #Evaluate the performance of  models using 3 metrics of Recall, Preceision and f1 score.\n",
        "  ypred = myKMeans(Xtrain,ytrain).predict(Xtest)\n",
        "  KMeans_recall = recall_score(ytest, ypred)\n",
        "  KMeans_precision = precision_score(ytest, ypred)\n",
        "  KMeans_f1 = f1_score(ytest, ypred)\n",
        "  KMeans_Results=[KMeans_recall, KMeans_precision, KMeans_f1 ]\n",
        "\n",
        "  ypred = myPCAKMeans(Xtrain,ytrain).predict(Xtest)\n",
        "  PCAKMeans_recall = recall_score(ytest, ypred)\n",
        "  PCAKMeans_precision = precision_score(ytest, ypred)\n",
        "  PCAKMeans_f1 = f1_score(ytest, ypred)\n",
        "  PCAKMeans_Results=[PCAKMeans_recall, PCAKMeans_precision, PCAKMeans_f1 ]\n",
        "\n",
        "  ypred = myPCARF(Xtrain,ytrain).predict(Xtest)\n",
        "  PCARF_recall = recall_score(ytest, ypred)\n",
        "  PCARF_precision = precision_score(ytest, ypred)\n",
        "  PCARF_f1 = f1_score(ytest, ypred)\n",
        "  PCARF_Results=[PCARF_recall, PCARF_precision, PCARF_f1 ]\n",
        "\n",
        "  return KMeans_Results, PCAKMeans_Results, PCARF_Results\n",
        "\n",
        "#Call the function here\n",
        "KMeans_Results, PCAKMeans_Results, PCARF_Results=myEvaluateUnsupervisedModelPerformance(Xtrain, Xtest, ytrain, ytest)\n",
        "print(\"KMeans_Results: \", KMeans_Results)\n",
        "print(\"PCAKMeans_Results: \", PCAKMeans_Results)\n",
        "print(\"PCARF_Results: \", PCARF_Results, \"\\n\")\n",
        "\n",
        "#print out the confusion matrix of the three models on the test set.\n",
        "printConfusionMatrix(myKMeans(Xtrain,ytrain))\n",
        "printConfusionMatrix(myPCAKMeans(Xtrain,ytrain))\n",
        "printConfusionMatrix(myPCARF(Xtrain,ytrain))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tLUT1nMwdYKw"
      },
      "source": [
        "#### Q3E)\n",
        "\n",
        "##Comparison of  the performance and training time of KMeans model and the Random Forest model \n",
        "\n",
        "\n",
        "*   Training Time Comaprison:\n",
        "\n",
        "KMeans  training time:  2.2316997051239014, \n",
        "myRandomForest_training_time:  0.4558906555175781 \n",
        "\n",
        "The KMeans training time is significantly higher than the random forest training time. This might be due to the complexity of KMeans algorithm.\n",
        "\n",
        "*   Performance comparison:\n",
        "KMeans_Results:  [Recall=0.15, Precision=0.012508017960230917, F1=0.023090586145648313]\n",
        "Random Forest Results:  [Recall=0.882051282051282, Precision=0.9927849927849928, F1=0.9341479972844535] \n",
        "\n",
        "The metrics of Kmeans model is extremely bad while that of Random Forest is very good, this might be because teh recall, precision and F1 scores are typically used to evaluate the performance of classifiers, not clustering algorithms.\n",
        "\n",
        "##Comparison of the performance and training time of PCA + Random Forest model from 3C and the Random Forest model \n",
        "\n",
        "*   Training Time Comaprison:\n",
        "\n",
        "PCARF  training time:  2.909113883972168 , \n",
        "myRandomForest_training_time:  0.4558906555175781\n",
        "\n",
        "The PCARF training time is significantly higher than the random forest training time. This might be due to the complexity of PCARF algorithm.\n",
        "\n",
        "*   Performance comparison:\n",
        "PCARF_Results:  [Recall= 0.4371794871794872, Precision=0.6595744680851063, F1= 0.525828835774865]\n",
        "Random Forest Results:  [Recall=0.882051282051282, Precision=0.9927849927849928, F1=0.9341479972844535] \n",
        "\n",
        "All the the 3 metrics of PCARF model is bad while that of Random Forest is very good. \n",
        "\n",
        "The reason for performance difference could be that, one is classifier while other is a clustering model\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5JYpQgsvd0bg"
      },
      "source": [
        "# Make sure to add sufficient comments to your code. "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}